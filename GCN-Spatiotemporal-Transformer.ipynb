{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:09.365685Z",
     "start_time": "2025-04-16T02:25:04.610709Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from networkx.classes import number_of_nodes\n",
    "\n",
    "import codes.data_utils\n",
    "import importlib\n",
    "\n",
    "importlib.reload(codes.data_utils)\n",
    "from codes.data_utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "# Load centrality data\n",
    "centrality_df = pd.read_excel(\"data/Centrality_indices.xlsx\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:10.873169Z",
     "start_time": "2025-04-16T02:25:10.826578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clean_cities_df = pd.read_csv(\"data/clean_non_flagged_cities.csv\")\n",
    "clean_city_ids = set(clean_cities_df['Codmundv'].astype(int).unique())"
   ],
   "id": "5fa0c6d394f585ae",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:15.374287Z",
     "start_time": "2025-04-16T02:25:15.347560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import codes.extract_backbone\n",
    "\n",
    "importlib.reload(codes.extract_backbone)\n",
    "from codes.extract_backbone import extract_backbone_from_files_brazil\n",
    "\n",
    "backbone_df = extract_backbone_from_files_brazil(\n",
    "    centrality_path=\"data/Centrality_indices.xlsx\",\n",
    "    mobility_edges_path=\"data/Road_and_waterway_connections_database_2016.xlsx\",\n",
    "    alpha=0.01,\n",
    "    city_whitelist=clean_city_ids\n",
    ")\n",
    "\n",
    "print(f\"Backbone extracted with {len(backbone_df)} edges\")\n",
    "print(backbone_df[['source', 'target', 'weekly_flow', 'pij']].head())\n"
   ],
   "id": "aa9e6fdba880c8c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Backbone file found at 'data/mobility_backbone_brazil.csv'. Loading it...\n",
      "Backbone extracted with 4811 edges\n",
      "    source   target  weekly_flow       pij\n",
      "0  1503903  1506005         14.0  0.598992\n",
      "1  1301902  1506005         12.5  0.406169\n",
      "2  1301902  1506807         13.5  0.115288\n",
      "3  2302107  2305100          0.0  1.000000\n",
      "4  3520509  3552403          0.0  1.000000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:22.547760Z",
     "start_time": "2025-04-16T02:25:19.701090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from codes.load_and_save_covid_data import load_and_save_covid_data\n",
    "\n",
    "covid_df = load_and_save_covid_data()\n",
    "\n",
    "print(f\"Full date range: {covid_df['date'].min()} to {covid_df['date'].max()}\")\n",
    "print(f\"Total records: {len(covid_df):,}\")\n",
    "\n",
    "negative_counts = (covid_df.select_dtypes(include=[np.number]) < 0).sum()\n",
    "print(\"üìâ Negative values per column:\")\n",
    "print(negative_counts[negative_counts > 0])\n"
   ],
   "id": "885bcc961fe9d06f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Found saved COVID dataset at data/covid_brazil_combined.csv. Loading it...\n",
      "Full date range: 2020-02-25 00:00:00 to 2023-03-18 00:00:00\n",
      "Total records: 5,830,987\n",
      "üìâ Negative values per column:\n",
      "newDeaths                          16629\n",
      "deaths                               186\n",
      "newCases                           63752\n",
      "totalCases                           278\n",
      "deaths_per_100k_inhabitants          186\n",
      "totalCases_per_100k_inhabitants      278\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:26.367616Z",
     "start_time": "2025-04-16T02:25:25.254339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from codes.preprocess_covid_brazil import filter_and_scale_covid_by_centrality\n",
    "\n",
    "filtered_scaled_covid_df = filter_and_scale_covid_by_centrality(covid_df, city_whitelist=clean_city_ids)"
   ],
   "id": "5d2f71fd71754ffe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Found saved preprocessed COVID data at 'data/filtered_scaled_covid.csv'. Loading...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:34.673522Z",
     "start_time": "2025-04-16T02:25:33.317640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from codes.graph_utils import build_pyg_graph_from_backbone\n",
    "\n",
    "centrality_df = centrality_df[centrality_df['Codmundv'].isin(clean_city_ids)].copy()\n",
    "pyg_data = build_pyg_graph_from_backbone(centrality_df, backbone_df)\n",
    "print(pyg_data)"
   ],
   "id": "821b693cfbda2660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Graph built with 1305 nodes and 4811 edges.\n",
      "[‚úì] Converted to PyTorch Geometric format.\n",
      "Data(edge_index=[2, 9622], name=[1305], weight=[9622], edge_attr=[9622, 1], num_nodes=1305)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:37.943104Z",
     "start_time": "2025-04-16T02:25:37.911655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using cpu')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "id": "9d269c25eed321f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:25:43.273739Z",
     "start_time": "2025-04-16T02:25:42.891310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from codes.data_utils import prepare_temporal_graph_data_non_overlapping, generate_sliding_temporal_graph_data, \\\n",
    "    prepare_temporal_graph_data_non_overlapping\n",
    "\n",
    "# Traditional (non-sliding) approach\n",
    "X_train_static, X_test_static, Y_train_static, Y_test_static = prepare_temporal_graph_data_non_overlapping(\n",
    "    filtered_scaled_covid_df,\n",
    "    sequence_length=15,\n",
    "    feature_column=\"z_newCases\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Sliding window approach\n",
    "X_train_slide, X_test_slide, Y_train_slide, Y_test_slide = generate_sliding_temporal_graph_data(\n",
    "    filtered_scaled_covid_df,\n",
    "    input_window=14,\n",
    "    output_window=1,\n",
    "    feature_column=\"z_newCases\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "Y_train_slide = Y_train_slide.squeeze(1)\n",
    "print(Y_train_slide.shape)\n",
    "\n",
    "Y_test_slide = Y_test_slide.squeeze(1)\n",
    "print(Y_test_slide.shape)\n",
    "\n",
    "# Compare number of samples\n",
    "total_static = len(X_train_static) + len(X_test_static)\n",
    "total_slide = len(X_train_slide) + len(X_test_slide)\n",
    "\n",
    "print(\"üìä Sample Count Comparison\")\n",
    "print(f\"Static window (15 input, 1 output): {total_static} samples\")\n",
    "print(f\"Sliding window (10 input, 1 output): {total_slide} samples\")\n",
    "print(f\"‚¨ÜÔ∏è Gain: {total_slide - total_static} samples (+{100 * (total_slide - total_static) / total_static:.2f}%)\")\n"
   ],
   "id": "4ebf4e03762a7cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[üìâ] (Non-overlapping) X shape: torch.Size([69, 15, 1305, 1]) | Y shape: torch.Size([69, 1305, 1])\n",
      "[üìâ] Train: torch.Size([55, 15, 1305, 1]) | Test: torch.Size([14, 15, 1305, 1])\n",
      "[‚úì] Sliding window: X torch.Size([1104, 14, 1305, 1]), Y torch.Size([1104, 1, 1305, 1])\n",
      "[‚úì] Train: torch.Size([883, 14, 1305, 1]), Test: torch.Size([221, 14, 1305, 1])\n",
      "torch.Size([883, 1305, 1])\n",
      "torch.Size([221, 1305, 1])\n",
      "üìä Sample Count Comparison\n",
      "Static window (15 input, 1 output): 69 samples\n",
      "Sliding window (10 input, 1 output): 1104 samples\n",
      "‚¨ÜÔ∏è Gain: 1035 samples (+1500.00%)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:26:09.575729Z",
     "start_time": "2025-04-16T02:26:08.870928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import codes.models.custom_gcn_spatiotemporal_transformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "importlib.reload(codes.models.custom_gcn_spatiotemporal_transformer)\n",
    "\n",
    "from codes.models.custom_gcn_spatiotemporal_transformer import SpatioTemporalFusionNet\n",
    "\n",
    "N = pyg_data.num_nodes\n",
    "\n",
    "model = SpatioTemporalFusionNet(\n",
    "    in_channels=1,           # number of time-series features per node (e.g., newCases)\n",
    "    graph_feat_dim=0,        # number of static node features (e.g., population, centrality, etc.)\n",
    "    trans_hidden=64,         # hidden dim for transformer and GCN\n",
    "    out_channels=1,          # output features per node (e.g., predicting next-day cases)\n",
    "    num_nodes=N              # number of nodes in the graph used for graph embeddings\n",
    ").to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # reduce LR by half every 5 epochs"
   ],
   "id": "e40d01862d55541a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:26:13.812817Z",
     "start_time": "2025-04-16T02:26:13.810398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CovidGraphDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ],
   "id": "3d8f7c05b8ce0540",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T02:26:17.486920Z",
     "start_time": "2025-04-16T02:26:17.484566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32  # You can tune this based on your GPU\n",
    "\n",
    "train_dataset = CovidGraphDataset(X_train_slide, Y_train_slide)\n",
    "test_dataset = CovidGraphDataset(X_test_slide, Y_test_slide)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "id": "771219ac58256ff0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-16T02:26:23.891742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "edge_index = pyg_data.edge_index.to(device)\n",
    "edge_weight = pyg_data.edge_attr.view(-1).to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "    for batch_X, batch_Y in pbar:\n",
    "        batch_X = batch_X.to(device)  # [B, T, N, F]\n",
    "        batch_Y = batch_Y.to(device)  # [B, N, 1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X, edge_index, edge_weight)  # [B, N, 1]\n",
    "        loss = criterion(output, batch_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{current_lr:.6f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"üß† Epoch {epoch+1}/{num_epochs} ‚Äî Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step()"
   ],
   "id": "966cdd645a1086a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9207a5787994539b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
